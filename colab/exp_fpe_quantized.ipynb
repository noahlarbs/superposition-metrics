{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["\n<br>\n", "Fixed Parameter Expansion (FPE) experiment based on Subramanian et al. (2025).<br>\n", "Extends the strong superposition baseline (weight_decay=-1) with an intervention at step 4000:<br>\n", "split polysemantic neurons and quantize the resulting expanded neurons using <br>\n", "BitNet 1.58b absmean ternary quantization for weights and 8-bit max-scaling for activations.<br>\n", "Trains a full-weight FPE model as a baseline target, then trains the quantized model<br>\n", "until it matches the target validation loss.<br>\n", ""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import torch\n", "from torch import nn\n", "import math\n", "import argparse\n", "import os\n", "import copy\n", "from adamw import AdamW\n", "from pr_dim import compute_pr_dim"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Threshold for \"non-zero\" weight (for identifying connections to partition)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["WEIGHT_THRESHOLD = 1e-6"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def weight_quant(w):\n", "    \"\"\"BitNet 1.58b weight quantization (absmean ternary) with STE.\"\"\"\n", "    scale = w.abs().mean().clamp(min=1e-8)\n", "    w_q = torch.round(w / scale).clamp(-1, 1) * scale\n", "    return w + (w_q - w).detach()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def activation_quant(x):\n", "    \"\"\"8-bit max-scaling activation quantization with STE.\"\"\"\n", "    scale = 127.0 / x.abs().max().clamp(min=1e-8)\n", "    x_q = torch.round(x * scale).clamp(-128, 127) / scale\n", "    return x + (x_q - x).detach()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class FeatureRecoveryQuantized(nn.Module):\n", "    \"\"\"FeatureRecovery that exposes hidden layer activations for PR dimension computation,\n", "    and selectively applies BitNet 1.58b quantization to expanded neurons.\n", "    \"\"\"\n", "    def __init__(self, n, m, W=None, b=None, expanded_indices=None):\n", "        super(FeatureRecoveryQuantized, self).__init__()\n", "        self.n = n\n", "        self.m = m\n", "        if W is not None:\n", "            self.W = nn.Parameter(W.clone())\n", "        else:\n", "            self.W = nn.Parameter(torch.randn(n, m) / math.sqrt(m))\n", "        if b is not None:\n", "            self.b = nn.Parameter(b.clone())\n", "        else:\n", "            self.b = nn.Parameter(torch.randn(n))\n", "        self.relu = nn.ReLU()\n", "        self.expanded_indices = expanded_indices if expanded_indices is not None else []\n", "    def forward(self, x, return_hidden=False):\n", "        if len(self.expanded_indices) > 0:\n", "            mask = torch.zeros(self.m, device=self.W.device, dtype=torch.bool)\n", "            mask[self.expanded_indices] = True\n", "            \n", "            W_exp_q = weight_quant(self.W[:, mask])\n", "            W_continuous = self.W[:, ~mask]\n", "            \n", "            hidden_exp = activation_quant(x) @ W_exp_q\n", "            hidden_cont = x @ W_continuous\n", "            \n", "            hidden = torch.empty(x.shape[0], self.m, device=x.device, dtype=x.dtype)\n", "            hidden[:, mask] = hidden_exp\n", "            hidden[:, ~mask] = hidden_cont\n", "            \n", "            hidden_for_out = hidden.clone()\n", "            hidden_for_out[:, mask] = activation_quant(hidden[:, mask])\n", "            \n", "            output = self.relu(hidden_for_out[:, mask] @ W_exp_q.T + hidden_for_out[:, ~mask] @ W_continuous.T + self.b)\n", "        else:\n", "            hidden = x @ self.W\n", "            output = self.relu(hidden @ self.W.T + self.b)\n", "            \n", "        if return_hidden:\n", "            return output, hidden\n", "        return output"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def split_polysemantic_neurons(W, n_children=2, reference_W=None):\n", "    \"\"\"\n", "    Split polysemantic neurons by partitioning each parent's non-zero weights\n", "    disjointly across children. Maintains total non-zero parameter count.\n", "    Returns:\n", "        W_new: [n, m_new] expanded weight matrix\n", "        split_indices: list of old indices that were split\n", "        expanded_indices: list of new indices belonging to the expanded children\n", "    \"\"\"\n", "    if reference_W is None:\n", "        reference_W = W\n", "    device = W.device\n", "    n, m = W.shape\n", "    new_columns = []\n", "    split_indices = []\n", "    expanded_indices = []\n", "    idx_counter = 0\n", "    for j in range(m):\n", "        w = W[:, j]\n", "        ref_w = reference_W[:, j]\n", "        nonzero_idx = (ref_w.abs() > WEIGHT_THRESHOLD).nonzero(as_tuple=True)[0]\n", "        if len(nonzero_idx) <= 1:\n", "            # Monosemantic: keep as single neuron\n", "            new_columns.append(w.unsqueeze(1))\n", "            idx_counter += 1\n", "        else:\n", "            # Polysemantic: partition connections disjointly across children\n", "            split_indices.append(j)\n", "            idx_list = nonzero_idx.tolist()\n", "            n_conn = len(idx_list)\n", "            n_splits = min(n_children, n_conn)\n", "            base_size = n_conn // n_splits\n", "            remainder = n_conn % n_splits\n", "            offset = 0\n", "            for k in range(n_splits):\n", "                size = base_size + (1 if k < remainder else 0)\n", "                if size == 0:\n", "                    continue\n", "                part_idx = idx_list[offset : offset + size]\n", "                offset += size\n", "                w_child = torch.zeros(n, device=device, dtype=W.dtype)\n", "                for i in part_idx:\n", "                    w_child[i] = w[i]\n", "                new_columns.append(w_child.unsqueeze(1))\n", "                expanded_indices.append(idx_counter)\n", "                idx_counter += 1\n", "    if not new_columns:\n", "        return W, [], []\n", "    W_new = torch.cat(new_columns, dim=1)\n", "    return W_new, split_indices, expanded_indices"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def get_lr(step, lr, n_steps, warmup_steps=2000):\n", "    step = step + 1\n", "    min_lr = 0.05 * lr\n", "    if warmup_steps < n_steps:\n", "        if step < warmup_steps:\n", "            return lr * step / warmup_steps\n", "        else:\n", "            return (lr - min_lr) * 0.5 * (\n", "                1 + math.cos(math.pi * (step - warmup_steps) / (n_steps - warmup_steps))\n", "            ) + min_lr\n", "    else:\n", "        return (lr - min_lr) * 0.5 * (1 + math.cos(math.pi * step / n_steps)) + min_lr"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def main():\n", "    parser = argparse.ArgumentParser(description=\"FPE: Quantized Extended Parameter Expansion experiment\")\n", "    parser.add_argument(\"--n\", type=int, default=1000, help=\"number of features\")\n", "    parser.add_argument(\"--m\", type=int, default=50, help=\"hidden dimension (pre-split)\")\n", "    parser.add_argument(\"--batch_size\", type=int, default=2048, help=\"batch size\")\n", "    parser.add_argument(\"--n_steps_pre\", type=int, default=4000, help=\"steps before FPE intervention\")\n", "    parser.add_argument(\"--n_steps_post\", type=int, default=2000, help=\"steps after FPE intervention\")\n", "    parser.add_argument(\"--max_extra_steps\", type=int, default=5000, help=\"max steps to run quantized model to match target loss\")\n", "    parser.add_argument(\"--log_interval\", type=int, default=500, help=\"log every N steps\")\n", "    parser.add_argument(\"--eval_batch_size\", type=int, default=8192, help=\"batch size for evaluating target loss\")\n", "    parser.add_argument(\"--alpha\", type=float, default=0.0, help=\"feature distribution power-law exponent\")\n", "    parser.add_argument(\"--output\", type=str, default=None, help=\"output path. If not provided, dynamically generated.\")\n", "    args = parser.parse_args()\n", "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n", "    print(f\"Using device: {device}\")\n", "    if args.output is None:\n", "        args.output = f\"../outputs/exp_fpe_quantized_n{args.n}_m{args.m}_pre{args.n_steps_pre}_post{args.n_steps_post}.pt\"\n\n", "    # Feature distribution\n", "    prob = torch.tensor([1.0 / i ** (1 + args.alpha) for i in range(1, args.n + 1)])\n", "    prob = prob / prob.sum()\n", "    prob = prob.to(device)\n", "    n_steps_total = args.n_steps_pre + args.n_steps_post\n", "    split_step = args.n_steps_pre\n\n", "    # Strong superposition pre-intervention base model\n", "    model = FeatureRecoveryQuantized(args.n, args.m).to(device)\n", "    parameter_groups = [\n", "        {\"params\": model.W, \"weight_decay\": -1.0},\n", "        {\"params\": model.b, \"weight_decay\": 0.0},\n", "    ]\n", "    optimizer = AdamW(parameter_groups, lr=1e-2)\n", "    criteria = nn.MSELoss()\n", "    losses = []\n", "    pr_dims = []\n\n", "    # Baseline tracking\n", "    baseline_model = FeatureRecoveryQuantized(args.n, args.m).to(device)\n", "    baseline_model.load_state_dict(model.state_dict())\n", "    baseline_optimizer = AdamW([\n", "        {\"params\": baseline_model.W, \"weight_decay\": -1.0},\n", "        {\"params\": baseline_model.b, \"weight_decay\": 0.0},\n", "    ], lr=1e-2)\n", "    \n", "    losses_baseline = []\n", "    pr_dims_baseline = []\n", "    print(f\"\\n--- Phase 1: Strong superposition (steps 0\u2013{split_step}) ---\")\n", "    for step in range(split_step):\n", "        x = (\n", "            (torch.rand(args.batch_size, args.n, device=device) < prob)\n", "            * torch.rand(args.batch_size, args.n, device=device)\n", "            * 2\n", "        )\n", "        lr = get_lr(step, 1e-2, split_step)\n", "        for pg in optimizer.param_groups:\n", "            pg[\"lr\"] = lr\n", "        optimizer.zero_grad()\n", "        y, hidden = model(x, return_hidden=True)\n", "        loss = criteria(y, x)\n", "        loss.backward()\n", "        optimizer.step()\n\n", "        # Baseline step\n", "        baseline_optimizer.zero_grad()\n", "        y_b, hidden_b = baseline_model(x, return_hidden=True)\n", "        loss_b = criteria(y_b, x)\n", "        loss_b.backward()\n", "        baseline_optimizer.step()\n", "        with torch.no_grad():\n", "            pr_dim = compute_pr_dim(hidden.detach()).item()\n", "            pr_dim_b = compute_pr_dim(hidden_b.detach()).item()\n", "        losses.append(loss.item())\n", "        pr_dims.append(pr_dim)\n", "        losses_baseline.append(loss_b.item())\n", "        pr_dims_baseline.append(pr_dim_b)\n", "        if (step + 1) % args.log_interval == 0:\n", "            print(f\"  step {step + 1}/{split_step} | loss: {loss.item():.4e} D_PR: {pr_dim:.2f} | Base loss: {loss_b.item():.4e} D_PR: {pr_dim_b:.2f}\")\n", "    print(f\"\\n  [Before FPE] step {split_step} | loss: {losses[-1]:.4e} | D_PR: {pr_dims[-1]:.2f}\")\n\n", "    # === FPE INTERVENTION: Split polysemantic neurons ===\n", "    W_old = model.W.detach()\n", "    m_old = W_old.shape[1]\n", "    nnz_before = (W_old.abs() > WEIGHT_THRESHOLD).sum().item()\n", "    W_new, split_indices, expanded_indices = split_polysemantic_neurons(W_old)\n", "    n_split = len(split_indices)\n", "    m_new = W_new.shape[1]\n", "    nnz_after = (W_new.abs() > WEIGHT_THRESHOLD).sum().item()\n", "    print(f\"\\n--- FPE intervention at step {split_step} ---\")\n", "    print(f\"  Hidden Layer Neurons: {m_old} -> {m_new} ({n_split} neurons split)\")\n", "    print(f\"  Total expanded (quantized) neurons: {len(expanded_indices)}\")\n", "    print(f\"  Non-zeros: {nnz_before} -> {nnz_after} (sparsity preserved)\")\n\n", "    # 1. Build FULL WEIGHT expanded model\n", "    model_full = FeatureRecoveryQuantized(args.n, m_new, W=W_new, b=model.b.detach(), expanded_indices=[]).to(device)\n", "    optimizer_full = AdamW([\n", "        {\"params\": model_full.W, \"weight_decay\": -1.0},\n", "        {\"params\": model_full.b, \"weight_decay\": 0.0},\n", "    ], lr=1e-2)\n\n", "    # 2. Build QUANTIZED expanded model\n", "    model_quant = FeatureRecoveryQuantized(args.n, m_new, W=W_new, b=model.b.detach(), expanded_indices=expanded_indices).to(device)\n", "    optimizer_quant = AdamW([\n", "        {\"params\": model_quant.W, \"weight_decay\": -1.0},\n", "        {\"params\": model_quant.b, \"weight_decay\": 0.0},\n", "    ], lr=1e-2)\n\n", "    # Initialize optimizers to set up dummy states (so we can copy state dict safely)\n", "    for opt, mod in [(optimizer_full, model_full), (optimizer_quant, model_quant)]:\n", "        opt.zero_grad()\n", "        mod(torch.zeros(1, args.n, device=device)).sum().backward()\n", "        opt.step()\n\n", "    # Safely map momenum state from W_old to W_new\n", "    if len(optimizer.state) > 0:\n", "        old_state_W = optimizer.state[model.W]\n", "        if 'exp_avg' in old_state_W:\n", "            base_exp_avg, _, _ = split_polysemantic_neurons(old_state_W['exp_avg'], reference_W=W_old)\n", "            base_exp_avg_sq, _, _ = split_polysemantic_neurons(old_state_W['exp_avg_sq'], reference_W=W_old)\n", "            \n", "            for opt, mod in [(optimizer_full, model_full), (optimizer_quant, model_quant)]:\n", "                opt.state[mod.W]['exp_avg'] = base_exp_avg.clone()\n", "                opt.state[mod.W]['exp_avg_sq'] = base_exp_avg_sq.clone()\n", "                opt.state[mod.W]['step'] = old_state_W['step']\n", "                \n", "                opt.state[mod.b]['exp_avg'] = optimizer.state[model.b]['exp_avg'].clone()\n", "                opt.state[mod.b]['exp_avg_sq'] = optimizer.state[model.b]['exp_avg_sq'].clone()\n", "                opt.state[mod.b]['step'] = optimizer.state[model.b]['step']"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["    # Enforce sparsity \n", "    sparsity_mask = (model_full.W.abs() > 0).float()\n", "    losses_full = list(losses)\n", "    pr_dims_full = list(pr_dims)\n", "    losses_quant = list(losses)\n", "    pr_dims_quant = list(pr_dims)\n", "    print(f\"\\n--- Phase 2: Post-FPE Synchronous Training (steps {split_step+1}\u2013{n_steps_total}) ---\")\n", "    for step in range(split_step, n_steps_total):\n", "        x = (\n", "            (torch.rand(args.batch_size, args.n, device=device) < prob)\n", "            * torch.rand(args.batch_size, args.n, device=device)\n", "            * 2\n", "        )\n", "        lr = get_lr(step, 1e-2, n_steps_total)\n", "        for pg in optimizer_full.param_groups:\n", "            pg[\"lr\"] = lr\n", "        for pg in optimizer_quant.param_groups:\n", "            pg[\"lr\"] = lr\n", "        for pg in baseline_optimizer.param_groups:\n", "            pg[\"lr\"] = lr\n\n", "        # Baseline step\n", "        baseline_optimizer.zero_grad()\n", "        y_b, hidden_b = baseline_model(x, return_hidden=True)\n", "        loss_b = criteria(y_b, x)\n", "        loss_b.backward()\n", "        baseline_optimizer.step()\n\n", "        # Full-weight step\n", "        optimizer_full.zero_grad()\n", "        y_f, hidden_f = model_full(x, return_hidden=True)\n", "        loss_f = criteria(y_f, x)\n", "        loss_f.backward()\n", "        model_full.W.grad.data *= sparsity_mask\n", "        optimizer_full.step()\n\n", "        # Quantized step\n", "        optimizer_quant.zero_grad()\n", "        y_q, hidden_q = model_quant(x, return_hidden=True)\n", "        loss_q = criteria(y_q, x)\n", "        loss_q.backward()\n", "        model_quant.W.grad.data *= sparsity_mask\n", "        optimizer_quant.step()\n", "        with torch.no_grad():\n", "            pr_dim_f = compute_pr_dim(hidden_f.detach()).item()\n", "            pr_dim_q = compute_pr_dim(hidden_q.detach()).item()\n", "            pr_dim_b = compute_pr_dim(hidden_b.detach()).item()\n", "        losses_full.append(loss_f.item())\n", "        pr_dims_full.append(pr_dim_f)\n", "        losses_quant.append(loss_q.item())\n", "        pr_dims_quant.append(pr_dim_q)\n", "        losses_baseline.append(loss_b.item())\n", "        pr_dims_baseline.append(pr_dim_b)\n", "        if (step + 1) % args.log_interval == 0:\n", "            print(f\"  step {step + 1}/{n_steps_total} | Full loss: {loss_f.item():.4e} D_PR: {pr_dim_f:.2f} | Quant loss: {loss_q.item():.4e} D_PR: {pr_dim_q:.2f} | Base loss: {loss_b.item():.4e} D_PR: {pr_dim_b:.2f}\")\n\n", "    # Evaluate target validation loss\n", "    print(\"\\n--- Evaluating Target Loss for Quantized Catch-up ---\")\n", "    x_eval = (torch.rand(args.eval_batch_size, args.n, device=device) < prob) * torch.rand(args.eval_batch_size, args.n, device=device) * 2\n", "    \n", "    with torch.no_grad():\n", "        target_loss = criteria(model_full(x_eval), x_eval).item()\n", "        quant_loss = criteria(model_quant(x_eval), x_eval).item()\n", "    print(f\"  Target loss (Full-weight): {target_loss:.6e}\")\n", "    print(f\"  Current loss (Quantized):  {quant_loss:.6e}\")\n\n", "    # Phase 3: Catch-up training for Quantized model\n", "    extra_steps = 0\n", "    final_catchup_loss = quant_loss\n", "    \n", "    if quant_loss > target_loss:\n", "        print(f\"\\n--- Phase 3: Running quantized model until val loss <= {target_loss:.6e} ---\")\n", "        \n", "        # Keep LR at minimum LR (end of cosine schedule)\n", "        min_lr = get_lr(n_steps_total - 1, 1e-2, n_steps_total)\n", "        for pg in optimizer_quant.param_groups:\n", "            pg[\"lr\"] = min_lr\n", "            \n", "        while final_catchup_loss > target_loss and extra_steps < args.max_extra_steps:\n", "            x = (\n", "                (torch.rand(args.batch_size, args.n, device=device) < prob)\n", "                * torch.rand(args.batch_size, args.n, device=device)\n", "                * 2\n", "            )\n", "            optimizer_quant.zero_grad()\n", "            y_q, hidden_q = model_quant(x, return_hidden=True)\n", "            loss_q = criteria(y_q, x)\n", "            loss_q.backward()\n", "            model_quant.W.grad.data *= sparsity_mask\n", "            optimizer_quant.step()\n", "            \n", "            with torch.no_grad():\n", "                pr_dim_q = compute_pr_dim(hidden_q.detach()).item()\n", "                \n", "            losses_quant.append(loss_q.item())\n", "            pr_dims_quant.append(pr_dim_q)\n", "            \n", "            extra_steps += 1\n", "            \n", "            if extra_steps % (args.log_interval // 5) == 0:\n", "                with torch.no_grad():\n", "                    # Evaluate strictly to compare\n", "                    curr_eval_loss = criteria(model_quant(x_eval), x_eval).item()\n", "                print(f\"  extra step {extra_steps} | Eval loss: {curr_eval_loss:.4e} D_PR: {pr_dim_q:.2f}\")\n", "                final_catchup_loss = curr_eval_loss\n", "                \n", "        if final_catchup_loss <= target_loss:\n", "            print(f\"\\n  Success! Quantized model matched target loss after {extra_steps} extra steps.\")\n", "        else:\n", "            print(f\"\\n  Stopped: Quantized model failed to match target loss within {args.max_extra_steps} extra steps.\")\n", "    else:\n", "         print(\"\\n  Quantized model already matched target loss! No extra steps needed.\")\n", "    \n", "    print(\"\\n--- Final Summary ---\")\n", "    print(f\"  Baseline:  loss={losses_baseline[-1]:.4e}, D_PR={pr_dims_baseline[-1]:.2f}\")\n", "    print(f\"  Full FPE:  loss={losses_full[-1]:.4e}, D_PR={pr_dims_full[-1]:.2f}\")\n", "    print(f\"  Quant FPE (step {n_steps_total + extra_steps}): loss={final_catchup_loss:.4e} (eval_loss), D_PR={pr_dims_quant[-1]:.2f}\")\n", "    os.makedirs(os.path.dirname(args.output) or \".\", exist_ok=True)\n", "    torch.save(\n", "        {\n", "            \"args\": vars(args),\n", "            \"losses_full\": losses_full,\n", "            \"pr_dims_full\": pr_dims_full,\n", "            \"losses_quant\": losses_quant,\n", "            \"pr_dims_quant\": pr_dims_quant,\n", "            \"losses_baseline\": losses_baseline,\n", "            \"pr_dims_baseline\": pr_dims_baseline,\n", "            \"extra_steps\": extra_steps,\n", "            \"target_loss\": target_loss,\n", "            \"final_catchup_loss\": final_catchup_loss,\n", "            \"m_before\": m_old,\n", "            \"m_after\": m_new,\n", "        },\n", "        args.output,\n", "    )\n", "    print(f\"\\nResults saved to {args.output}\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["if __name__ == \"__main__\":\n", "    main()"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}